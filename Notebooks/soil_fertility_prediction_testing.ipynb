{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.9\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B  \\\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11   \n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29   \n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03   \n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82   \n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26   \n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69   \n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40   \n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32   \n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53   \n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63   \n",
       "\n",
       "     Output  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "..      ...  \n",
       "875       1  \n",
       "876       1  \n",
       "877       1  \n",
       "878       2  \n",
       "879       0  \n",
       "\n",
       "[880 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[[\"Output\"]]\n",
    "feature_matrix = data.drop(\"Output\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11\n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29\n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03\n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82\n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26\n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...\n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69\n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40\n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32\n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53\n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63\n",
       "\n",
       "[880 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data[\"Output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_matrix_scaled = MaxAbsScaler().fit_transform(feature_matrix)\n",
    "feature_matrix_scaled = feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11\n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29\n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03\n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82\n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26\n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...\n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69\n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40\n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32\n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53\n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63\n",
       "\n",
       "[880 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix_scaled, data[\"Output\"], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>257</td>\n",
       "      <td>7.5</td>\n",
       "      <td>887</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>345</td>\n",
       "      <td>76.8</td>\n",
       "      <td>676</td>\n",
       "      <td>7.10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.27</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>320</td>\n",
       "      <td>12.3</td>\n",
       "      <td>676</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.96</td>\n",
       "      <td>11.05</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>201</td>\n",
       "      <td>8.3</td>\n",
       "      <td>697</td>\n",
       "      <td>7.48</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.30</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>201</td>\n",
       "      <td>89.9</td>\n",
       "      <td>454</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.21</td>\n",
       "      <td>2.01</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>69</td>\n",
       "      <td>4.4</td>\n",
       "      <td>507</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.39</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.03</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>282</td>\n",
       "      <td>7.0</td>\n",
       "      <td>401</td>\n",
       "      <td>7.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.67</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>339</td>\n",
       "      <td>7.5</td>\n",
       "      <td>475</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.98</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.54</td>\n",
       "      <td>7.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>295</td>\n",
       "      <td>7.0</td>\n",
       "      <td>433</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.49</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.01</td>\n",
       "      <td>6.36</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>295</td>\n",
       "      <td>4.8</td>\n",
       "      <td>327</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.42</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC     S    Zn    Fe    Cu     Mn     B\n",
       "290  257   7.5  887  7.50  0.29  0.79  4.50  0.26  2.84  0.67   6.76  0.12\n",
       "685  345  76.8  676  7.10  0.70  0.38  8.20  0.34  0.83  0.27   6.13  0.32\n",
       "547  320  12.3  676  7.90  0.50  0.59  9.95  0.38  8.24  0.96  11.05  0.27\n",
       "835  201   8.3  697  7.48  0.81  0.20  6.64  0.53  5.60  0.86  11.30  0.63\n",
       "798  201  89.9  454  7.43  0.53  0.78  6.94  0.55  8.21  2.01   7.54  0.17\n",
       "..   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...    ...   ...\n",
       "106   69   4.4  507  7.70  0.53  0.10  5.13  0.39  4.02  1.03  13.05  0.25\n",
       "270  282   7.0  401  7.71  0.48  0.18  2.50  0.24  2.48  0.67   7.64  0.10\n",
       "860  339   7.5  475  7.00  0.88  0.98  6.94  0.54  7.15  0.39  10.59  0.34\n",
       "435  295   7.0  433  7.41  0.35  0.49  8.45  0.48  2.05  2.01   6.36  0.61\n",
       "102  295   4.8  327  7.60  0.43  0.88  3.32  0.42  7.63  0.51   9.03  0.86\n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = y_train.values.ravel()\n",
    "y_test_new = y_test.values.ravel()\n",
    "out_new = out.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1, 1, 2,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 2,\n",
       "       2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 1, 2, 2, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 1,\n",
       "       1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8920454545454546"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 75, learning_rate = 0.12, max_depth = 7, min_samples_leaf=7, subsample=0.6, random_state=0)\n",
    "my_model = model.fit(X_train, y_train_new)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy = accuracy_score(y_test_new, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8920454545454546"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grad = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.93        78\n",
      "           1       0.90      0.89      0.89        88\n",
      "           2       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.89       176\n",
      "   macro avg       0.93      0.72      0.76       176\n",
      "weighted avg       0.90      0.89      0.88       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_new, y_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(my_model.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(12,)),\n",
    "        tf.keras.layers.Dense(units = 15, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
    "        tf.keras.layers.Dense(units = 6, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
    "        tf.keras.layers.Dense(units = 3, activation = 'softmax'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 15)                195       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 96        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 312\n",
      "Trainable params: 312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.9\\lib\\site-packages\\keras\\backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/22 [===============>..............] - ETA: 0s - loss: 5.6754 - accuracy: 0.4089   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.9\\lib\\site-packages\\keras\\backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 6s 37ms/step - loss: 3.7204 - accuracy: 0.4574 - val_loss: 1.2827 - val_accuracy: 0.4943\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1.1781 - accuracy: 0.4688 - val_loss: 1.0658 - val_accuracy: 0.5000\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9907 - accuracy: 0.5653 - val_loss: 0.9635 - val_accuracy: 0.6648\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9138 - accuracy: 0.6463 - val_loss: 0.9474 - val_accuracy: 0.6193\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8251 - accuracy: 0.7415 - val_loss: 0.9487 - val_accuracy: 0.6534\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.7733 - accuracy: 0.7315 - val_loss: 0.8517 - val_accuracy: 0.6818\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7273 - accuracy: 0.7585 - val_loss: 0.8130 - val_accuracy: 0.6875\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.7107 - accuracy: 0.7528 - val_loss: 0.8327 - val_accuracy: 0.6761\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6898 - accuracy: 0.7571 - val_loss: 0.7850 - val_accuracy: 0.6875\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6660 - accuracy: 0.7670 - val_loss: 0.7578 - val_accuracy: 0.7045\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6461 - accuracy: 0.7727 - val_loss: 0.7597 - val_accuracy: 0.7330\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6264 - accuracy: 0.8011 - val_loss: 0.7244 - val_accuracy: 0.7386\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.6270 - accuracy: 0.7784 - val_loss: 0.7555 - val_accuracy: 0.7216\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 1s 22ms/step - loss: 0.6086 - accuracy: 0.8011 - val_loss: 0.7227 - val_accuracy: 0.7273\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5932 - accuracy: 0.8082 - val_loss: 0.7049 - val_accuracy: 0.7386\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5940 - accuracy: 0.8011 - val_loss: 0.7009 - val_accuracy: 0.7273\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5810 - accuracy: 0.8068 - val_loss: 0.7140 - val_accuracy: 0.7443\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5637 - accuracy: 0.8253 - val_loss: 0.7404 - val_accuracy: 0.7557\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5671 - accuracy: 0.8295 - val_loss: 0.7217 - val_accuracy: 0.7386\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5591 - accuracy: 0.8097 - val_loss: 0.6641 - val_accuracy: 0.7443\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5611 - accuracy: 0.8111 - val_loss: 0.6766 - val_accuracy: 0.7443\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5593 - accuracy: 0.8224 - val_loss: 0.7325 - val_accuracy: 0.6932\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5502 - accuracy: 0.8253 - val_loss: 0.6482 - val_accuracy: 0.7557\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5410 - accuracy: 0.8324 - val_loss: 0.6531 - val_accuracy: 0.7500\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.8551 - val_loss: 0.6454 - val_accuracy: 0.7614\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5283 - accuracy: 0.8324 - val_loss: 0.7236 - val_accuracy: 0.7670\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5222 - accuracy: 0.8409 - val_loss: 0.6824 - val_accuracy: 0.7273\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5429 - accuracy: 0.8295 - val_loss: 0.7190 - val_accuracy: 0.6989\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5374 - accuracy: 0.8267 - val_loss: 0.6343 - val_accuracy: 0.7841\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5201 - accuracy: 0.8395 - val_loss: 0.6360 - val_accuracy: 0.7727\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5080 - accuracy: 0.8381 - val_loss: 0.6630 - val_accuracy: 0.7386\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5167 - accuracy: 0.8338 - val_loss: 0.6551 - val_accuracy: 0.7386\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.5340 - accuracy: 0.8239 - val_loss: 0.6550 - val_accuracy: 0.7500\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.5178 - accuracy: 0.8338 - val_loss: 0.6267 - val_accuracy: 0.7784\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.4969 - accuracy: 0.8480 - val_loss: 0.6405 - val_accuracy: 0.7670\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4856 - accuracy: 0.8608 - val_loss: 0.6133 - val_accuracy: 0.7898\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4966 - accuracy: 0.8466 - val_loss: 0.6112 - val_accuracy: 0.7784\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4753 - accuracy: 0.8565 - val_loss: 0.6110 - val_accuracy: 0.7841\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.4954 - accuracy: 0.8594 - val_loss: 0.5954 - val_accuracy: 0.7955\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.8480 - val_loss: 0.6104 - val_accuracy: 0.7784\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.4872 - accuracy: 0.8381 - val_loss: 0.7043 - val_accuracy: 0.7557\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4843 - accuracy: 0.8651 - val_loss: 0.6010 - val_accuracy: 0.7955\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4645 - accuracy: 0.8651 - val_loss: 0.5916 - val_accuracy: 0.8068\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4625 - accuracy: 0.8551 - val_loss: 0.6068 - val_accuracy: 0.7784\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4635 - accuracy: 0.8565 - val_loss: 0.6264 - val_accuracy: 0.7557\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4524 - accuracy: 0.8707 - val_loss: 0.5915 - val_accuracy: 0.8011\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4406 - accuracy: 0.8736 - val_loss: 0.5843 - val_accuracy: 0.7955\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4484 - accuracy: 0.8764 - val_loss: 0.5987 - val_accuracy: 0.7841\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4671 - accuracy: 0.8523 - val_loss: 0.5805 - val_accuracy: 0.8011\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4364 - accuracy: 0.8750 - val_loss: 0.5682 - val_accuracy: 0.8125\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4340 - accuracy: 0.8707 - val_loss: 0.5864 - val_accuracy: 0.8011\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4403 - accuracy: 0.8693 - val_loss: 0.5777 - val_accuracy: 0.8011\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4296 - accuracy: 0.8736 - val_loss: 0.5601 - val_accuracy: 0.8068\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4566 - accuracy: 0.8580 - val_loss: 0.6102 - val_accuracy: 0.7500\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4240 - accuracy: 0.8736 - val_loss: 0.5503 - val_accuracy: 0.8125\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4242 - accuracy: 0.8878 - val_loss: 0.5392 - val_accuracy: 0.8125\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4237 - accuracy: 0.8636 - val_loss: 0.5460 - val_accuracy: 0.8068\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4103 - accuracy: 0.8835 - val_loss: 0.5416 - val_accuracy: 0.8182\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4231 - accuracy: 0.8693 - val_loss: 0.5576 - val_accuracy: 0.8011\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4143 - accuracy: 0.8807 - val_loss: 0.5247 - val_accuracy: 0.8125\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4298 - accuracy: 0.8736 - val_loss: 0.5428 - val_accuracy: 0.8295\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3976 - accuracy: 0.8991 - val_loss: 0.5407 - val_accuracy: 0.8068\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.4311 - accuracy: 0.8778 - val_loss: 0.6569 - val_accuracy: 0.7216\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4321 - accuracy: 0.8736 - val_loss: 0.5793 - val_accuracy: 0.8125\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4206 - accuracy: 0.8849 - val_loss: 0.5926 - val_accuracy: 0.7955\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4333 - accuracy: 0.8651 - val_loss: 0.5267 - val_accuracy: 0.8182\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4369 - accuracy: 0.8750 - val_loss: 0.5593 - val_accuracy: 0.8011\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4520 - accuracy: 0.8636 - val_loss: 0.5948 - val_accuracy: 0.7500\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4319 - accuracy: 0.8693 - val_loss: 0.5453 - val_accuracy: 0.8182\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4135 - accuracy: 0.8778 - val_loss: 0.5266 - val_accuracy: 0.8352\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4091 - accuracy: 0.8821 - val_loss: 0.5329 - val_accuracy: 0.8352\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4011 - accuracy: 0.8835 - val_loss: 0.5237 - val_accuracy: 0.8352\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4364 - accuracy: 0.8565 - val_loss: 0.5245 - val_accuracy: 0.8295\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4332 - accuracy: 0.8636 - val_loss: 0.5206 - val_accuracy: 0.8239\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.4411 - accuracy: 0.8509 - val_loss: 0.5464 - val_accuracy: 0.8182\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4067 - accuracy: 0.8892 - val_loss: 0.5467 - val_accuracy: 0.8182\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.4167 - accuracy: 0.8722 - val_loss: 0.5253 - val_accuracy: 0.8295\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.4163 - accuracy: 0.8878 - val_loss: 0.5205 - val_accuracy: 0.8352\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4135 - accuracy: 0.8807 - val_loss: 0.5130 - val_accuracy: 0.8182\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.3859 - accuracy: 0.8920 - val_loss: 0.5007 - val_accuracy: 0.8239\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.3990 - accuracy: 0.8821 - val_loss: 0.5620 - val_accuracy: 0.7898\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4473 - accuracy: 0.8537 - val_loss: 0.5084 - val_accuracy: 0.8239\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.4020 - accuracy: 0.8764 - val_loss: 0.5157 - val_accuracy: 0.8239\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4638 - accuracy: 0.8438 - val_loss: 0.6155 - val_accuracy: 0.7443\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4377 - accuracy: 0.8693 - val_loss: 0.5044 - val_accuracy: 0.8409\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.4519 - accuracy: 0.8466 - val_loss: 0.7393 - val_accuracy: 0.6761\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.4199 - accuracy: 0.8764 - val_loss: 0.5052 - val_accuracy: 0.8239\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.3948 - accuracy: 0.8920 - val_loss: 0.5155 - val_accuracy: 0.8239\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.3917 - accuracy: 0.8778 - val_loss: 0.5501 - val_accuracy: 0.8068\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4535 - accuracy: 0.8580 - val_loss: 0.4997 - val_accuracy: 0.8295\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4211 - accuracy: 0.8764 - val_loss: 0.5180 - val_accuracy: 0.8295\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3959 - accuracy: 0.8892 - val_loss: 0.4913 - val_accuracy: 0.8295\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.4140 - accuracy: 0.8750 - val_loss: 0.5250 - val_accuracy: 0.8182\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 0.3990 - accuracy: 0.8835 - val_loss: 0.5034 - val_accuracy: 0.8409\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3878 - accuracy: 0.8864 - val_loss: 0.4941 - val_accuracy: 0.8352\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3980 - accuracy: 0.8835 - val_loss: 0.4982 - val_accuracy: 0.8295\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3813 - accuracy: 0.9006 - val_loss: 0.5235 - val_accuracy: 0.8182\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3837 - accuracy: 0.8807 - val_loss: 0.5090 - val_accuracy: 0.8239\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3793 - accuracy: 0.8949 - val_loss: 0.4781 - val_accuracy: 0.8295\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3820 - accuracy: 0.8906 - val_loss: 0.4745 - val_accuracy: 0.8466\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3724 - accuracy: 0.8949 - val_loss: 0.5021 - val_accuracy: 0.8239\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3923 - accuracy: 0.8821 - val_loss: 0.4830 - val_accuracy: 0.8409\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3850 - accuracy: 0.8920 - val_loss: 0.4954 - val_accuracy: 0.8295\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3796 - accuracy: 0.8864 - val_loss: 0.5767 - val_accuracy: 0.8068\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 23ms/step - loss: 0.4393 - accuracy: 0.8622 - val_loss: 0.5200 - val_accuracy: 0.8125\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.4067 - accuracy: 0.8778 - val_loss: 0.4796 - val_accuracy: 0.8295\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.3950 - accuracy: 0.8651 - val_loss: 0.4987 - val_accuracy: 0.8239\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3767 - accuracy: 0.8977 - val_loss: 0.4978 - val_accuracy: 0.8239\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3913 - accuracy: 0.8935 - val_loss: 0.5085 - val_accuracy: 0.8295\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.4089 - accuracy: 0.8778 - val_loss: 0.5125 - val_accuracy: 0.8182\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.4175 - accuracy: 0.8750 - val_loss: 0.4893 - val_accuracy: 0.8409\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3843 - accuracy: 0.8963 - val_loss: 0.4753 - val_accuracy: 0.8466\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3925 - accuracy: 0.8849 - val_loss: 0.4713 - val_accuracy: 0.8352\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3856 - accuracy: 0.8793 - val_loss: 0.4621 - val_accuracy: 0.8466\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.4105 - accuracy: 0.8665 - val_loss: 0.5224 - val_accuracy: 0.8182\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.4186 - accuracy: 0.8651 - val_loss: 0.4854 - val_accuracy: 0.8295\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3705 - accuracy: 0.8864 - val_loss: 0.4652 - val_accuracy: 0.8295\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3707 - accuracy: 0.8935 - val_loss: 0.4779 - val_accuracy: 0.8295\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3662 - accuracy: 0.8935 - val_loss: 0.4731 - val_accuracy: 0.8295\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3607 - accuracy: 0.8963 - val_loss: 0.4513 - val_accuracy: 0.8352\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3835 - accuracy: 0.8807 - val_loss: 0.4759 - val_accuracy: 0.8352\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3883 - accuracy: 0.8778 - val_loss: 0.4545 - val_accuracy: 0.8523\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3529 - accuracy: 0.8935 - val_loss: 0.5140 - val_accuracy: 0.8068\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3625 - accuracy: 0.8920 - val_loss: 0.5013 - val_accuracy: 0.8182\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3696 - accuracy: 0.8835 - val_loss: 0.5205 - val_accuracy: 0.8239\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3970 - accuracy: 0.8764 - val_loss: 0.4535 - val_accuracy: 0.8352\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3768 - accuracy: 0.8892 - val_loss: 0.4657 - val_accuracy: 0.8466\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3682 - accuracy: 0.8892 - val_loss: 0.4780 - val_accuracy: 0.8409\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3724 - accuracy: 0.8892 - val_loss: 0.4528 - val_accuracy: 0.8409\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3845 - accuracy: 0.8935 - val_loss: 0.4490 - val_accuracy: 0.8352\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 1s 34ms/step - loss: 0.3526 - accuracy: 0.8963 - val_loss: 0.4622 - val_accuracy: 0.8466\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3657 - accuracy: 0.8849 - val_loss: 0.4534 - val_accuracy: 0.8466\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3698 - accuracy: 0.8849 - val_loss: 0.4535 - val_accuracy: 0.8523\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3563 - accuracy: 0.8892 - val_loss: 0.4465 - val_accuracy: 0.8466\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3695 - accuracy: 0.8920 - val_loss: 0.5223 - val_accuracy: 0.8295\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3897 - accuracy: 0.8736 - val_loss: 0.4521 - val_accuracy: 0.8636\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3803 - accuracy: 0.8793 - val_loss: 0.4585 - val_accuracy: 0.8352\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3984 - accuracy: 0.8693 - val_loss: 0.6181 - val_accuracy: 0.7273\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.4262 - accuracy: 0.8636 - val_loss: 0.5725 - val_accuracy: 0.7955\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 1s 37ms/step - loss: 0.3603 - accuracy: 0.8906 - val_loss: 0.4485 - val_accuracy: 0.8409\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 23ms/step - loss: 0.3781 - accuracy: 0.8821 - val_loss: 0.4522 - val_accuracy: 0.8352\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3623 - accuracy: 0.8920 - val_loss: 0.4478 - val_accuracy: 0.8523\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3552 - accuracy: 0.8935 - val_loss: 0.4918 - val_accuracy: 0.8239\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3605 - accuracy: 0.8920 - val_loss: 0.4653 - val_accuracy: 0.8352\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 23ms/step - loss: 0.3470 - accuracy: 0.8949 - val_loss: 0.4369 - val_accuracy: 0.8352\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3569 - accuracy: 0.8906 - val_loss: 0.4539 - val_accuracy: 0.8523\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3498 - accuracy: 0.8991 - val_loss: 0.4448 - val_accuracy: 0.8523\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3560 - accuracy: 0.8963 - val_loss: 0.4458 - val_accuracy: 0.8466\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.3466 - accuracy: 0.9034 - val_loss: 0.4389 - val_accuracy: 0.8409\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3835 - accuracy: 0.8849 - val_loss: 0.5050 - val_accuracy: 0.8182\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3792 - accuracy: 0.8750 - val_loss: 0.4501 - val_accuracy: 0.8352\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3488 - accuracy: 0.8892 - val_loss: 0.4414 - val_accuracy: 0.8409\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3474 - accuracy: 0.8977 - val_loss: 0.4396 - val_accuracy: 0.8523\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3514 - accuracy: 0.8963 - val_loss: 0.4345 - val_accuracy: 0.8409\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3537 - accuracy: 0.8920 - val_loss: 0.4412 - val_accuracy: 0.8352\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3456 - accuracy: 0.8977 - val_loss: 0.4454 - val_accuracy: 0.8580\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3438 - accuracy: 0.9048 - val_loss: 0.4390 - val_accuracy: 0.8409\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.3663 - accuracy: 0.8764 - val_loss: 0.4357 - val_accuracy: 0.8409\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3539 - accuracy: 0.8935 - val_loss: 0.4340 - val_accuracy: 0.8409\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3392 - accuracy: 0.9048 - val_loss: 0.4491 - val_accuracy: 0.8523\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3490 - accuracy: 0.8991 - val_loss: 0.4346 - val_accuracy: 0.8352\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3364 - accuracy: 0.9034 - val_loss: 0.4526 - val_accuracy: 0.8466\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3728 - accuracy: 0.8878 - val_loss: 0.4335 - val_accuracy: 0.8466\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3589 - accuracy: 0.8906 - val_loss: 0.5104 - val_accuracy: 0.8295\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3735 - accuracy: 0.8722 - val_loss: 0.4450 - val_accuracy: 0.8580\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3446 - accuracy: 0.9006 - val_loss: 0.4485 - val_accuracy: 0.8580\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3650 - accuracy: 0.8849 - val_loss: 0.4608 - val_accuracy: 0.8352\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3537 - accuracy: 0.8949 - val_loss: 0.4365 - val_accuracy: 0.8409\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3441 - accuracy: 0.9077 - val_loss: 0.4468 - val_accuracy: 0.8352\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3417 - accuracy: 0.9034 - val_loss: 0.4336 - val_accuracy: 0.8409\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3419 - accuracy: 0.9062 - val_loss: 0.4431 - val_accuracy: 0.8580\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3447 - accuracy: 0.8991 - val_loss: 0.4328 - val_accuracy: 0.8580\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.3731 - accuracy: 0.8849 - val_loss: 0.4377 - val_accuracy: 0.8409\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3798 - accuracy: 0.8722 - val_loss: 0.4573 - val_accuracy: 0.8239\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3836 - accuracy: 0.8764 - val_loss: 0.4490 - val_accuracy: 0.8636\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3497 - accuracy: 0.8920 - val_loss: 0.4339 - val_accuracy: 0.8409\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3331 - accuracy: 0.9034 - val_loss: 0.4427 - val_accuracy: 0.8580\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.3633 - accuracy: 0.8849 - val_loss: 0.5208 - val_accuracy: 0.8125\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3614 - accuracy: 0.8793 - val_loss: 0.4357 - val_accuracy: 0.8409\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3679 - accuracy: 0.8849 - val_loss: 0.4472 - val_accuracy: 0.8352\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3758 - accuracy: 0.8778 - val_loss: 0.5445 - val_accuracy: 0.7955\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4015 - accuracy: 0.8750 - val_loss: 0.4353 - val_accuracy: 0.8466\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3449 - accuracy: 0.8920 - val_loss: 0.4485 - val_accuracy: 0.8580\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3557 - accuracy: 0.8963 - val_loss: 0.4336 - val_accuracy: 0.8409\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3874 - accuracy: 0.8793 - val_loss: 0.4688 - val_accuracy: 0.8239\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4060 - accuracy: 0.8594 - val_loss: 0.4416 - val_accuracy: 0.8636\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3543 - accuracy: 0.8991 - val_loss: 0.4348 - val_accuracy: 0.8466\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3400 - accuracy: 0.9006 - val_loss: 0.4587 - val_accuracy: 0.8352\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4000 - accuracy: 0.8736 - val_loss: 0.4716 - val_accuracy: 0.8409\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3515 - accuracy: 0.8920 - val_loss: 0.4396 - val_accuracy: 0.8409\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3469 - accuracy: 0.8963 - val_loss: 0.4525 - val_accuracy: 0.8239\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3490 - accuracy: 0.9006 - val_loss: 0.4726 - val_accuracy: 0.8239\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3919 - accuracy: 0.8707 - val_loss: 0.4406 - val_accuracy: 0.8409\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3470 - accuracy: 0.9020 - val_loss: 0.4469 - val_accuracy: 0.8352\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3602 - accuracy: 0.8991 - val_loss: 0.4446 - val_accuracy: 0.8409\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3589 - accuracy: 0.8963 - val_loss: 0.4588 - val_accuracy: 0.8352\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.3952 - accuracy: 0.8835 - val_loss: 0.4689 - val_accuracy: 0.8295\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 1s 31ms/step - loss: 0.3484 - accuracy: 0.9077 - val_loss: 0.4433 - val_accuracy: 0.8580\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3605 - accuracy: 0.8920 - val_loss: 0.4480 - val_accuracy: 0.8580\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3507 - accuracy: 0.9006 - val_loss: 0.4380 - val_accuracy: 0.8466\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3398 - accuracy: 0.9048 - val_loss: 0.4335 - val_accuracy: 0.8466\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3593 - accuracy: 0.8878 - val_loss: 0.4423 - val_accuracy: 0.8466\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3450 - accuracy: 0.9062 - val_loss: 0.4518 - val_accuracy: 0.8352\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4015 - accuracy: 0.8665 - val_loss: 0.4778 - val_accuracy: 0.8239\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3695 - accuracy: 0.8807 - val_loss: 0.4448 - val_accuracy: 0.8409\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3648 - accuracy: 0.8949 - val_loss: 0.4531 - val_accuracy: 0.8352\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3486 - accuracy: 0.9034 - val_loss: 0.4374 - val_accuracy: 0.8466\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3428 - accuracy: 0.9020 - val_loss: 0.4447 - val_accuracy: 0.8580\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3437 - accuracy: 0.9048 - val_loss: 0.5375 - val_accuracy: 0.8011\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3547 - accuracy: 0.8977 - val_loss: 0.4556 - val_accuracy: 0.8523\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3676 - accuracy: 0.8935 - val_loss: 0.5032 - val_accuracy: 0.8295\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.4078 - accuracy: 0.8722 - val_loss: 0.4460 - val_accuracy: 0.8409\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3536 - accuracy: 0.9006 - val_loss: 0.4409 - val_accuracy: 0.8466\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3498 - accuracy: 0.9119 - val_loss: 0.4476 - val_accuracy: 0.8580\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.3426 - accuracy: 0.9105 - val_loss: 0.4492 - val_accuracy: 0.8636\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3495 - accuracy: 0.8977 - val_loss: 0.5180 - val_accuracy: 0.8125\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3730 - accuracy: 0.8864 - val_loss: 0.4455 - val_accuracy: 0.8523\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3777 - accuracy: 0.8722 - val_loss: 0.4411 - val_accuracy: 0.8466\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3456 - accuracy: 0.8935 - val_loss: 0.4379 - val_accuracy: 0.8409\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3556 - accuracy: 0.8949 - val_loss: 0.4486 - val_accuracy: 0.8352\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3415 - accuracy: 0.9020 - val_loss: 0.4465 - val_accuracy: 0.8409\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3567 - accuracy: 0.8835 - val_loss: 0.4357 - val_accuracy: 0.8352\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3451 - accuracy: 0.8963 - val_loss: 0.4816 - val_accuracy: 0.8352\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3492 - accuracy: 0.8963 - val_loss: 0.4439 - val_accuracy: 0.8580\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3391 - accuracy: 0.9105 - val_loss: 0.4418 - val_accuracy: 0.8409\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3491 - accuracy: 0.8878 - val_loss: 0.4379 - val_accuracy: 0.8466\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3490 - accuracy: 0.8977 - val_loss: 0.4801 - val_accuracy: 0.8352\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3368 - accuracy: 0.9062 - val_loss: 0.4385 - val_accuracy: 0.8466\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3334 - accuracy: 0.9062 - val_loss: 0.4565 - val_accuracy: 0.8409\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3383 - accuracy: 0.9062 - val_loss: 0.4546 - val_accuracy: 0.8409\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3601 - accuracy: 0.8892 - val_loss: 0.4551 - val_accuracy: 0.8295\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3392 - accuracy: 0.8949 - val_loss: 0.4266 - val_accuracy: 0.8523\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3917 - accuracy: 0.8665 - val_loss: 0.4353 - val_accuracy: 0.8352\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3493 - accuracy: 0.8949 - val_loss: 0.5098 - val_accuracy: 0.8182\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.3524 - accuracy: 0.8849 - val_loss: 0.4605 - val_accuracy: 0.8352\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3355 - accuracy: 0.9091 - val_loss: 0.4681 - val_accuracy: 0.8295\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.3652 - accuracy: 0.8849 - val_loss: 0.4545 - val_accuracy: 0.8352\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.3926 - accuracy: 0.8778 - val_loss: 0.4432 - val_accuracy: 0.8409\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3538 - accuracy: 0.8920 - val_loss: 0.4448 - val_accuracy: 0.8352\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.3528 - accuracy: 0.8963 - val_loss: 0.4873 - val_accuracy: 0.8352\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3575 - accuracy: 0.8935 - val_loss: 0.4500 - val_accuracy: 0.8580\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3443 - accuracy: 0.9020 - val_loss: 0.4677 - val_accuracy: 0.8352\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3359 - accuracy: 0.9020 - val_loss: 0.4411 - val_accuracy: 0.8580\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3393 - accuracy: 0.9006 - val_loss: 0.5745 - val_accuracy: 0.7841\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3449 - accuracy: 0.9062 - val_loss: 0.4347 - val_accuracy: 0.8352\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3451 - accuracy: 0.8991 - val_loss: 0.4387 - val_accuracy: 0.8409\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3407 - accuracy: 0.8991 - val_loss: 0.4553 - val_accuracy: 0.8409\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.4350 - accuracy: 0.8622 - val_loss: 0.5752 - val_accuracy: 0.7841\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3763 - accuracy: 0.8864 - val_loss: 0.4509 - val_accuracy: 0.8636\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3532 - accuracy: 0.9006 - val_loss: 0.5300 - val_accuracy: 0.8125\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3886 - accuracy: 0.8793 - val_loss: 0.5492 - val_accuracy: 0.7955\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3395 - accuracy: 0.9034 - val_loss: 0.4788 - val_accuracy: 0.8352\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3446 - accuracy: 0.8977 - val_loss: 0.4906 - val_accuracy: 0.8352\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3536 - accuracy: 0.8949 - val_loss: 0.4898 - val_accuracy: 0.8295\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.8977 - val_loss: 0.4523 - val_accuracy: 0.8409\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3706 - accuracy: 0.8821 - val_loss: 0.5335 - val_accuracy: 0.8068\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4284 - accuracy: 0.8509 - val_loss: 0.6315 - val_accuracy: 0.7443\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8707 - val_loss: 0.4872 - val_accuracy: 0.8352\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3525 - accuracy: 0.8991 - val_loss: 0.4932 - val_accuracy: 0.8352\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.9020 - val_loss: 0.4438 - val_accuracy: 0.8580\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3380 - accuracy: 0.8977 - val_loss: 0.4321 - val_accuracy: 0.8409\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.9020 - val_loss: 0.4434 - val_accuracy: 0.8636\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3367 - accuracy: 0.9062 - val_loss: 0.4411 - val_accuracy: 0.8580\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3412 - accuracy: 0.8963 - val_loss: 0.4517 - val_accuracy: 0.8409\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3909 - accuracy: 0.8778 - val_loss: 0.4508 - val_accuracy: 0.8523\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3534 - accuracy: 0.8991 - val_loss: 0.4867 - val_accuracy: 0.8352\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3509 - accuracy: 0.8977 - val_loss: 0.4716 - val_accuracy: 0.8295\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3424 - accuracy: 0.8991 - val_loss: 0.4414 - val_accuracy: 0.8466\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3441 - accuracy: 0.9077 - val_loss: 0.4621 - val_accuracy: 0.8409\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8920 - val_loss: 0.5226 - val_accuracy: 0.8125\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3805 - accuracy: 0.8750 - val_loss: 0.5264 - val_accuracy: 0.8125\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3425 - accuracy: 0.9006 - val_loss: 0.4467 - val_accuracy: 0.8580\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.9006 - val_loss: 0.4373 - val_accuracy: 0.8523\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3468 - accuracy: 0.8977 - val_loss: 0.4533 - val_accuracy: 0.8409\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3340 - accuracy: 0.9091 - val_loss: 0.4662 - val_accuracy: 0.8409\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3340 - accuracy: 0.9062 - val_loss: 0.4345 - val_accuracy: 0.8409\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3489 - accuracy: 0.9020 - val_loss: 0.4368 - val_accuracy: 0.8523\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3373 - accuracy: 0.9020 - val_loss: 0.4835 - val_accuracy: 0.8352\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3429 - accuracy: 0.8963 - val_loss: 0.4857 - val_accuracy: 0.8352\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3546 - accuracy: 0.8892 - val_loss: 0.4316 - val_accuracy: 0.8352\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3419 - accuracy: 0.9006 - val_loss: 0.4462 - val_accuracy: 0.8580\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3299 - accuracy: 0.9034 - val_loss: 0.4291 - val_accuracy: 0.8409\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3266 - accuracy: 0.9091 - val_loss: 0.4280 - val_accuracy: 0.8409\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3468 - accuracy: 0.8892 - val_loss: 0.4709 - val_accuracy: 0.8409\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3276 - accuracy: 0.9034 - val_loss: 0.5031 - val_accuracy: 0.8182\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3540 - accuracy: 0.8920 - val_loss: 0.4298 - val_accuracy: 0.8352\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3365 - accuracy: 0.9062 - val_loss: 0.4381 - val_accuracy: 0.8352\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3361 - accuracy: 0.9062 - val_loss: 0.4499 - val_accuracy: 0.8466\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3463 - accuracy: 0.8991 - val_loss: 0.4378 - val_accuracy: 0.8409\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3941 - accuracy: 0.8736 - val_loss: 0.4290 - val_accuracy: 0.8466\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5353 - accuracy: 0.8267 - val_loss: 0.4702 - val_accuracy: 0.8352\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3766 - accuracy: 0.8764 - val_loss: 0.4837 - val_accuracy: 0.8295\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3584 - accuracy: 0.8906 - val_loss: 0.4337 - val_accuracy: 0.8409\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.9006 - val_loss: 0.4267 - val_accuracy: 0.8466\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3262 - accuracy: 0.9006 - val_loss: 0.4575 - val_accuracy: 0.8409\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3334 - accuracy: 0.8949 - val_loss: 0.4283 - val_accuracy: 0.8580\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3296 - accuracy: 0.9048 - val_loss: 0.4254 - val_accuracy: 0.8409\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.9048 - val_loss: 0.4606 - val_accuracy: 0.8352\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8991 - val_loss: 0.4430 - val_accuracy: 0.8580\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3459 - accuracy: 0.8949 - val_loss: 0.4348 - val_accuracy: 0.8466\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3294 - accuracy: 0.9020 - val_loss: 0.4686 - val_accuracy: 0.8239\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3401 - accuracy: 0.8906 - val_loss: 0.4238 - val_accuracy: 0.8409\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.9105 - val_loss: 0.4219 - val_accuracy: 0.8466\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3414 - accuracy: 0.8906 - val_loss: 0.4244 - val_accuracy: 0.8523\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3418 - accuracy: 0.8949 - val_loss: 0.4653 - val_accuracy: 0.8352\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3909 - accuracy: 0.8864 - val_loss: 0.4416 - val_accuracy: 0.8352\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.8949 - val_loss: 0.4366 - val_accuracy: 0.8523\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3310 - accuracy: 0.9062 - val_loss: 0.4500 - val_accuracy: 0.8636\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3380 - accuracy: 0.9062 - val_loss: 0.4658 - val_accuracy: 0.8466\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3557 - accuracy: 0.8906 - val_loss: 0.4336 - val_accuracy: 0.8580\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.9119 - val_loss: 0.4304 - val_accuracy: 0.8466\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3368 - accuracy: 0.8935 - val_loss: 0.4348 - val_accuracy: 0.8580\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3286 - accuracy: 0.9048 - val_loss: 0.4416 - val_accuracy: 0.8580\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3682 - accuracy: 0.8821 - val_loss: 0.4756 - val_accuracy: 0.8352\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3882 - accuracy: 0.8807 - val_loss: 0.4547 - val_accuracy: 0.8466\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.4156 - accuracy: 0.8679 - val_loss: 0.5370 - val_accuracy: 0.8352\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.4710 - accuracy: 0.8480 - val_loss: 0.4553 - val_accuracy: 0.8352\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3727 - accuracy: 0.8849 - val_loss: 0.4417 - val_accuracy: 0.8409\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3419 - accuracy: 0.9020 - val_loss: 0.4532 - val_accuracy: 0.8523\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3638 - accuracy: 0.8892 - val_loss: 0.4454 - val_accuracy: 0.8352\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3537 - accuracy: 0.8949 - val_loss: 0.4444 - val_accuracy: 0.8466\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3336 - accuracy: 0.9148 - val_loss: 0.4445 - val_accuracy: 0.8409\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3476 - accuracy: 0.8963 - val_loss: 0.4350 - val_accuracy: 0.8409\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.9020 - val_loss: 0.4522 - val_accuracy: 0.8636\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3367 - accuracy: 0.9006 - val_loss: 0.4578 - val_accuracy: 0.8466\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3548 - accuracy: 0.8920 - val_loss: 0.4875 - val_accuracy: 0.8295\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3324 - accuracy: 0.9020 - val_loss: 0.4432 - val_accuracy: 0.8580\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3659 - accuracy: 0.8949 - val_loss: 0.4913 - val_accuracy: 0.8409\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3620 - accuracy: 0.8906 - val_loss: 0.4640 - val_accuracy: 0.8409\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3440 - accuracy: 0.9020 - val_loss: 0.4619 - val_accuracy: 0.8409\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3545 - accuracy: 0.8892 - val_loss: 0.4311 - val_accuracy: 0.8409\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3357 - accuracy: 0.8935 - val_loss: 0.4440 - val_accuracy: 0.8409\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8963 - val_loss: 0.4818 - val_accuracy: 0.8352\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3202 - accuracy: 0.9077 - val_loss: 0.4316 - val_accuracy: 0.8352\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3466 - accuracy: 0.8949 - val_loss: 0.4226 - val_accuracy: 0.8409\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3358 - accuracy: 0.8935 - val_loss: 0.4489 - val_accuracy: 0.8523\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.9006 - val_loss: 0.4212 - val_accuracy: 0.8409\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3266 - accuracy: 0.9034 - val_loss: 0.4246 - val_accuracy: 0.8409\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3292 - accuracy: 0.9006 - val_loss: 0.4264 - val_accuracy: 0.8409\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.8963 - val_loss: 0.4873 - val_accuracy: 0.8295\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3338 - accuracy: 0.8920 - val_loss: 0.4463 - val_accuracy: 0.8409\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.8977 - val_loss: 0.4298 - val_accuracy: 0.8636\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.9062 - val_loss: 0.4959 - val_accuracy: 0.8239\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3338 - accuracy: 0.9034 - val_loss: 0.4253 - val_accuracy: 0.8580\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3634 - accuracy: 0.8821 - val_loss: 0.4287 - val_accuracy: 0.8409\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8892 - val_loss: 0.4237 - val_accuracy: 0.8523\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.9062 - val_loss: 0.4656 - val_accuracy: 0.8295\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.8920 - val_loss: 0.4279 - val_accuracy: 0.8409\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3803 - accuracy: 0.8793 - val_loss: 0.4435 - val_accuracy: 0.8523\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3375 - accuracy: 0.8892 - val_loss: 0.4283 - val_accuracy: 0.8466\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3585 - accuracy: 0.8878 - val_loss: 0.4431 - val_accuracy: 0.8580\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3634 - accuracy: 0.8821 - val_loss: 0.4984 - val_accuracy: 0.8295\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3455 - accuracy: 0.8849 - val_loss: 0.4188 - val_accuracy: 0.8409\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.9048 - val_loss: 0.4175 - val_accuracy: 0.8466\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3312 - accuracy: 0.9048 - val_loss: 0.4303 - val_accuracy: 0.8466\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3222 - accuracy: 0.8949 - val_loss: 0.4166 - val_accuracy: 0.8409\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3619 - accuracy: 0.8864 - val_loss: 0.4608 - val_accuracy: 0.8239\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.9020 - val_loss: 0.4334 - val_accuracy: 0.8409\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3615 - accuracy: 0.8849 - val_loss: 0.5757 - val_accuracy: 0.8182\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3583 - accuracy: 0.8807 - val_loss: 0.4289 - val_accuracy: 0.8409\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3379 - accuracy: 0.8906 - val_loss: 0.4262 - val_accuracy: 0.8409\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3492 - accuracy: 0.8750 - val_loss: 0.4212 - val_accuracy: 0.8409\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3275 - accuracy: 0.8977 - val_loss: 0.4191 - val_accuracy: 0.8409\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.9034 - val_loss: 0.4862 - val_accuracy: 0.8295\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3258 - accuracy: 0.9034 - val_loss: 0.4284 - val_accuracy: 0.8466\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.9034 - val_loss: 0.4435 - val_accuracy: 0.8466\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3249 - accuracy: 0.8991 - val_loss: 0.4375 - val_accuracy: 0.8523\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3448 - accuracy: 0.8963 - val_loss: 0.4313 - val_accuracy: 0.8352\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3446 - accuracy: 0.8949 - val_loss: 0.4312 - val_accuracy: 0.8352\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3703 - accuracy: 0.8807 - val_loss: 0.4335 - val_accuracy: 0.8409\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3303 - accuracy: 0.9006 - val_loss: 0.4380 - val_accuracy: 0.8409\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3686 - accuracy: 0.8864 - val_loss: 0.4336 - val_accuracy: 0.8409\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3448 - accuracy: 0.8949 - val_loss: 0.4919 - val_accuracy: 0.8182\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4002 - accuracy: 0.8551 - val_loss: 0.4588 - val_accuracy: 0.8409\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3368 - accuracy: 0.8963 - val_loss: 0.4305 - val_accuracy: 0.8352\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3504 - accuracy: 0.8935 - val_loss: 0.4256 - val_accuracy: 0.8409\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3173 - accuracy: 0.9034 - val_loss: 0.4328 - val_accuracy: 0.8580\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.9034 - val_loss: 0.4290 - val_accuracy: 0.8352\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3163 - accuracy: 0.9062 - val_loss: 0.4283 - val_accuracy: 0.8352\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3389 - accuracy: 0.8963 - val_loss: 0.4405 - val_accuracy: 0.8352\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.8864 - val_loss: 0.4360 - val_accuracy: 0.8523\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3630 - accuracy: 0.8821 - val_loss: 0.4229 - val_accuracy: 0.8409\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3367 - accuracy: 0.8963 - val_loss: 0.4580 - val_accuracy: 0.8466\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3269 - accuracy: 0.9048 - val_loss: 0.4247 - val_accuracy: 0.8409\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3410 - accuracy: 0.8977 - val_loss: 0.4415 - val_accuracy: 0.8580\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3438 - accuracy: 0.8920 - val_loss: 0.5296 - val_accuracy: 0.8352\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3752 - accuracy: 0.8835 - val_loss: 0.4546 - val_accuracy: 0.8295\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3481 - accuracy: 0.8963 - val_loss: 0.6079 - val_accuracy: 0.7670\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3595 - accuracy: 0.8849 - val_loss: 0.4342 - val_accuracy: 0.8352\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3727 - accuracy: 0.8665 - val_loss: 0.4486 - val_accuracy: 0.8295\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4457 - accuracy: 0.8565 - val_loss: 0.4837 - val_accuracy: 0.8352\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3547 - accuracy: 0.8949 - val_loss: 0.4547 - val_accuracy: 0.8409\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3338 - accuracy: 0.8963 - val_loss: 0.4776 - val_accuracy: 0.8352\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3338 - accuracy: 0.8991 - val_loss: 0.4324 - val_accuracy: 0.8466\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3218 - accuracy: 0.9119 - val_loss: 0.4221 - val_accuracy: 0.8466\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3306 - accuracy: 0.9034 - val_loss: 0.4591 - val_accuracy: 0.8466\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9062 - val_loss: 0.4437 - val_accuracy: 0.8466\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3480 - accuracy: 0.8935 - val_loss: 0.5122 - val_accuracy: 0.8239\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3417 - accuracy: 0.8935 - val_loss: 0.4259 - val_accuracy: 0.8580\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3251 - accuracy: 0.8949 - val_loss: 0.4751 - val_accuracy: 0.8295\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3310 - accuracy: 0.8977 - val_loss: 0.4316 - val_accuracy: 0.8352\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3270 - accuracy: 0.9006 - val_loss: 0.4207 - val_accuracy: 0.8352\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3230 - accuracy: 0.9020 - val_loss: 0.4138 - val_accuracy: 0.8466\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3338 - accuracy: 0.8892 - val_loss: 0.4259 - val_accuracy: 0.8409\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3338 - accuracy: 0.8949 - val_loss: 0.4248 - val_accuracy: 0.8466\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3718 - accuracy: 0.8778 - val_loss: 0.5012 - val_accuracy: 0.8295\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3559 - accuracy: 0.8807 - val_loss: 0.4283 - val_accuracy: 0.8409\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3216 - accuracy: 0.8977 - val_loss: 0.4421 - val_accuracy: 0.8352\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3401 - accuracy: 0.8935 - val_loss: 0.4300 - val_accuracy: 0.8352\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3258 - accuracy: 0.8963 - val_loss: 0.4356 - val_accuracy: 0.8523\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3149 - accuracy: 0.9077 - val_loss: 0.4379 - val_accuracy: 0.8466\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.8991 - val_loss: 0.4383 - val_accuracy: 0.8636\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.9034 - val_loss: 0.4292 - val_accuracy: 0.8352\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3363 - accuracy: 0.8963 - val_loss: 0.4497 - val_accuracy: 0.8523\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3246 - accuracy: 0.8991 - val_loss: 0.4172 - val_accuracy: 0.8409\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.8977 - val_loss: 0.4199 - val_accuracy: 0.8523\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3184 - accuracy: 0.9048 - val_loss: 0.4144 - val_accuracy: 0.8523\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3235 - accuracy: 0.9062 - val_loss: 0.4216 - val_accuracy: 0.8409\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3417 - accuracy: 0.8977 - val_loss: 0.4177 - val_accuracy: 0.8409\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.9020 - val_loss: 0.4183 - val_accuracy: 0.8466\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3209 - accuracy: 0.9077 - val_loss: 0.4634 - val_accuracy: 0.8295\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3723 - accuracy: 0.8693 - val_loss: 0.4242 - val_accuracy: 0.8466\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3261 - accuracy: 0.9034 - val_loss: 0.4868 - val_accuracy: 0.8182\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3535 - accuracy: 0.8864 - val_loss: 0.4503 - val_accuracy: 0.8409\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3697 - accuracy: 0.8793 - val_loss: 0.5482 - val_accuracy: 0.8295\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3874 - accuracy: 0.8679 - val_loss: 0.4616 - val_accuracy: 0.8352\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8835 - val_loss: 0.4516 - val_accuracy: 0.8409\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.9034 - val_loss: 0.4202 - val_accuracy: 0.8409\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3239 - accuracy: 0.9006 - val_loss: 0.4226 - val_accuracy: 0.8409\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3235 - accuracy: 0.9020 - val_loss: 0.4332 - val_accuracy: 0.8352\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3425 - accuracy: 0.8906 - val_loss: 0.4336 - val_accuracy: 0.8466\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3798 - accuracy: 0.8750 - val_loss: 0.5059 - val_accuracy: 0.8352\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.9034 - val_loss: 0.4705 - val_accuracy: 0.8352\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.9020 - val_loss: 0.4148 - val_accuracy: 0.8352\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3203 - accuracy: 0.9048 - val_loss: 0.4256 - val_accuracy: 0.8523\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3241 - accuracy: 0.8963 - val_loss: 0.4630 - val_accuracy: 0.8295\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3389 - accuracy: 0.8849 - val_loss: 0.4250 - val_accuracy: 0.8409\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8991 - val_loss: 0.4211 - val_accuracy: 0.8352\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3307 - accuracy: 0.9020 - val_loss: 0.4669 - val_accuracy: 0.8295\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3676 - accuracy: 0.8750 - val_loss: 0.4264 - val_accuracy: 0.8466\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.9020 - val_loss: 0.4225 - val_accuracy: 0.8523\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.9062 - val_loss: 0.4199 - val_accuracy: 0.8523\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.9034 - val_loss: 0.4203 - val_accuracy: 0.8523\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.9048 - val_loss: 0.4192 - val_accuracy: 0.8409\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.9077 - val_loss: 0.4270 - val_accuracy: 0.8580\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3178 - accuracy: 0.9048 - val_loss: 0.4147 - val_accuracy: 0.8352\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3343 - accuracy: 0.8935 - val_loss: 0.4199 - val_accuracy: 0.8409\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8977 - val_loss: 0.4197 - val_accuracy: 0.8352\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3399 - accuracy: 0.8963 - val_loss: 0.4905 - val_accuracy: 0.8239\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3841 - accuracy: 0.8622 - val_loss: 0.4310 - val_accuracy: 0.8466\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3670 - accuracy: 0.8807 - val_loss: 0.4294 - val_accuracy: 0.8409\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.9105 - val_loss: 0.4270 - val_accuracy: 0.8409\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3249 - accuracy: 0.8977 - val_loss: 0.4225 - val_accuracy: 0.8409\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.9020 - val_loss: 0.5438 - val_accuracy: 0.8068\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3719 - accuracy: 0.8807 - val_loss: 0.4807 - val_accuracy: 0.8352\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3249 - accuracy: 0.8906 - val_loss: 0.4189 - val_accuracy: 0.8466\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.8935 - val_loss: 0.4707 - val_accuracy: 0.8295\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3672 - accuracy: 0.8807 - val_loss: 0.4410 - val_accuracy: 0.8352\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3253 - accuracy: 0.8920 - val_loss: 0.4181 - val_accuracy: 0.8466\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3192 - accuracy: 0.8991 - val_loss: 0.4654 - val_accuracy: 0.8295\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3268 - accuracy: 0.9020 - val_loss: 0.4249 - val_accuracy: 0.8580\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.9006 - val_loss: 0.4447 - val_accuracy: 0.8409\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3428 - accuracy: 0.8864 - val_loss: 0.4322 - val_accuracy: 0.8409\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8963 - val_loss: 0.4174 - val_accuracy: 0.8523\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3300 - accuracy: 0.9006 - val_loss: 0.4282 - val_accuracy: 0.8409\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.9020 - val_loss: 0.4260 - val_accuracy: 0.8409\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3291 - accuracy: 0.8949 - val_loss: 0.4288 - val_accuracy: 0.8580\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3329 - accuracy: 0.8935 - val_loss: 0.4235 - val_accuracy: 0.8636\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3855 - accuracy: 0.8679 - val_loss: 0.6685 - val_accuracy: 0.8011\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.4232 - accuracy: 0.8537 - val_loss: 0.4512 - val_accuracy: 0.8409\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3558 - accuracy: 0.8778 - val_loss: 0.4220 - val_accuracy: 0.8409\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3335 - accuracy: 0.8949 - val_loss: 0.4224 - val_accuracy: 0.8523\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3396 - accuracy: 0.8920 - val_loss: 0.4677 - val_accuracy: 0.8295\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3251 - accuracy: 0.8920 - val_loss: 0.4617 - val_accuracy: 0.8409\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.3654 - accuracy: 0.8764 - val_loss: 0.4482 - val_accuracy: 0.8409\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4020 - accuracy: 0.8693 - val_loss: 0.4349 - val_accuracy: 0.8409\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3494 - accuracy: 0.8977 - val_loss: 0.4388 - val_accuracy: 0.8409\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3377 - accuracy: 0.8935 - val_loss: 0.4388 - val_accuracy: 0.8466\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3259 - accuracy: 0.9048 - val_loss: 0.4244 - val_accuracy: 0.8409\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3270 - accuracy: 0.9048 - val_loss: 0.4491 - val_accuracy: 0.8409\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3288 - accuracy: 0.8977 - val_loss: 0.4259 - val_accuracy: 0.8466\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.4248 - accuracy: 0.8693 - val_loss: 0.6126 - val_accuracy: 0.7898\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3800 - accuracy: 0.8807 - val_loss: 0.4644 - val_accuracy: 0.8295\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3522 - accuracy: 0.8991 - val_loss: 0.5505 - val_accuracy: 0.8068\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4070 - accuracy: 0.8793 - val_loss: 0.5471 - val_accuracy: 0.8068\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3392 - accuracy: 0.9020 - val_loss: 0.4319 - val_accuracy: 0.8352\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.3262 - accuracy: 0.9048 - val_loss: 0.4316 - val_accuracy: 0.8466\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8991 - val_loss: 0.4335 - val_accuracy: 0.8409\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3314 - accuracy: 0.9034 - val_loss: 0.4326 - val_accuracy: 0.8352\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3532 - accuracy: 0.8906 - val_loss: 0.4447 - val_accuracy: 0.8409\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3332 - accuracy: 0.8949 - val_loss: 0.4620 - val_accuracy: 0.8466\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3314 - accuracy: 0.9006 - val_loss: 0.4986 - val_accuracy: 0.8295\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3368 - accuracy: 0.8935 - val_loss: 0.4380 - val_accuracy: 0.8523\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.3513 - accuracy: 0.8935 - val_loss: 0.5092 - val_accuracy: 0.8295\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4056 - accuracy: 0.8651 - val_loss: 0.4488 - val_accuracy: 0.8409\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3355 - accuracy: 0.9006 - val_loss: 0.4403 - val_accuracy: 0.8409\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.3283 - accuracy: 0.9077 - val_loss: 0.4243 - val_accuracy: 0.8409\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3370 - accuracy: 0.9020 - val_loss: 0.4291 - val_accuracy: 0.8409\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3479 - accuracy: 0.8949 - val_loss: 0.4361 - val_accuracy: 0.8409\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.9062 - val_loss: 0.4429 - val_accuracy: 0.8523\n"
     ]
    }
   ],
   "source": [
    "my_model2 = model_2.fit(X_train, y_train_new, epochs=500, validation_data=(X_test, y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(my_model2.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147727489471436\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(my_model2.history)[\"accuracy\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, model_2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "response = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7028682e-02, 9.3871635e-01, 3.4254920e-02],\n",
       "       [8.5116255e-01, 1.4712547e-01, 1.7119157e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [8.3006166e-02, 8.9172512e-01, 2.5268719e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [5.8594836e-05, 8.8098121e-01, 1.1896023e-01],\n",
       "       [9.0870344e-08, 6.6143805e-01, 3.3856186e-01],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.1962376e-03, 9.4228208e-01, 5.4521658e-02],\n",
       "       [9.1284674e-01, 8.6271942e-02, 8.8135147e-04],\n",
       "       [1.6943304e-04, 9.0240109e-01, 9.7429439e-02],\n",
       "       [1.6214781e-04, 9.0158874e-01, 9.8249100e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.9343811e-03, 9.4382787e-01, 5.2237775e-02],\n",
       "       [3.2804921e-01, 6.5894192e-01, 1.3008917e-02],\n",
       "       [9.7563617e-02, 8.7847275e-01, 2.3963647e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [6.3282187e-04, 9.2389804e-01, 7.5469062e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.1440822e-03, 9.3174374e-01, 6.7112200e-02],\n",
       "       [9.5958868e-03, 9.4711661e-01, 4.3287497e-02],\n",
       "       [4.2246990e-02, 9.2710888e-01, 3.0644057e-02],\n",
       "       [1.9639989e-05, 8.5483098e-01, 1.4514934e-01],\n",
       "       [4.7724831e-01, 5.1388234e-01, 8.8693574e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [4.2984862e-02, 9.2650920e-01, 3.0505901e-02],\n",
       "       [2.1540280e-03, 9.3875390e-01, 5.9092123e-02],\n",
       "       [1.3618778e-01, 8.4261250e-01, 2.1199729e-02],\n",
       "       [6.5328732e-02, 9.0749109e-01, 2.7180189e-02],\n",
       "       [1.4091750e-04, 8.9895254e-01, 1.0090651e-01],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.7388767e-02, 8.7863302e-01, 2.3978228e-02],\n",
       "       [7.0404480e-05, 8.8495773e-01, 1.1497196e-01],\n",
       "       [8.6214399e-04, 9.2814070e-01, 7.0997171e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.8336067e-02, 9.4415832e-01, 3.7505664e-02],\n",
       "       [8.6299786e-03, 9.4708323e-01, 4.4286855e-02],\n",
       "       [7.0181035e-02, 9.0320873e-01, 2.6610248e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [5.1172622e-02, 9.1970849e-01, 2.9118808e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [7.2229075e-01, 2.7393362e-01, 3.7756735e-03],\n",
       "       [4.1491997e-01, 5.7460976e-01, 1.0470261e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.5716549e-02, 9.4544637e-01, 3.8837079e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.5925442e-01, 8.2088941e-01, 1.9856177e-02],\n",
       "       [1.1394061e-02, 9.4690311e-01, 4.1702833e-02],\n",
       "       [4.9178624e-01, 4.9969637e-01, 8.5173650e-03],\n",
       "       [1.7750962e-02, 9.4446534e-01, 3.7783671e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.2543312e-03, 9.3285573e-01, 6.5889955e-02],\n",
       "       [7.8850240e-01, 2.0883353e-01, 2.6640294e-03],\n",
       "       [2.6772500e-03, 9.4078785e-01, 5.6534950e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [8.5114425e-01, 1.4714351e-01, 1.7121802e-03],\n",
       "       [5.9804332e-01, 3.9580551e-01, 6.1510862e-03],\n",
       "       [1.6930453e-02, 9.4487822e-01, 3.8191345e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.0329773e-01, 8.7320429e-01, 2.3498030e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.0808344e-01, 9.0975441e-02, 9.4115839e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.3952535e-01, 8.3948058e-01, 2.0994140e-02],\n",
       "       [2.3379058e-04, 9.0816289e-01, 9.1603361e-02],\n",
       "       [5.5518411e-02, 9.1600972e-01, 2.8471900e-02],\n",
       "       [1.6859702e-04, 9.0231007e-01, 9.7521394e-02],\n",
       "       [2.0613978e-02, 9.4287550e-01, 3.6510497e-02],\n",
       "       [2.6053974e-01, 7.2412795e-01, 1.5332302e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.8957950e-03, 9.4375992e-01, 5.2344300e-02],\n",
       "       [1.2149452e-03, 9.3247348e-01, 6.6311561e-02],\n",
       "       [9.5112948e-03, 9.4711840e-01, 4.3370280e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [4.6246085e-01, 5.2830392e-01, 9.2352657e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.2211750e-05, 8.4209269e-01, 1.5789513e-01],\n",
       "       [8.9648329e-02, 8.8570267e-01, 2.4649041e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [2.7299512e-03, 9.4095963e-01, 5.6310430e-02],\n",
       "       [6.0109311e-04, 9.2316234e-01, 7.6236598e-02],\n",
       "       [1.4609310e-01, 8.3330560e-01, 2.0601314e-02],\n",
       "       [3.3805603e-01, 6.4924979e-01, 1.2694224e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.3378954e-02, 9.4636494e-01, 4.0256225e-02],\n",
       "       [2.8582150e-02, 9.3762100e-01, 3.3796795e-02],\n",
       "       [4.6892394e-05, 8.7599850e-01, 1.2395461e-01],\n",
       "       [3.1251419e-01, 6.7397475e-01, 1.3511090e-02],\n",
       "       [1.5944665e-03, 9.3561178e-01, 6.2793791e-02],\n",
       "       [1.4014471e-01, 8.3889878e-01, 2.0956444e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [8.0226040e-01, 1.9529322e-01, 2.4464247e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.6812950e-01, 8.1248885e-01, 1.9381572e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.3937308e-01, 6.4797366e-01, 1.2653289e-02],\n",
       "       [6.5452820e-01, 3.4044874e-01, 5.0230860e-03],\n",
       "       [6.2584704e-01, 3.6856750e-01, 5.5854623e-03],\n",
       "       [1.4063333e-04, 8.9891422e-01, 1.0094516e-01],\n",
       "       [9.4388509e-01, 5.5601884e-02, 5.1300158e-04],\n",
       "       [1.6089682e-03, 9.3571150e-01, 6.2679499e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.9931607e-01, 5.8978683e-01, 1.0897029e-02],\n",
       "       [3.4317501e-02, 9.3337083e-01, 3.2311626e-02],\n",
       "       [2.1025872e-01, 7.7237415e-01, 1.7367151e-02],\n",
       "       [6.3999963e-01, 3.5469505e-01, 5.3053326e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.8605276e-03, 9.4369674e-01, 5.2442722e-02],\n",
       "       [6.6265696e-01, 3.3247545e-01, 4.8674876e-03],\n",
       "       [1.6297139e-03, 9.3585223e-01, 6.2518083e-02],\n",
       "       [5.8782208e-01, 4.0581369e-01, 6.3642571e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.1867194e-01, 6.6801798e-01, 1.3309979e-02],\n",
       "       [4.8789945e-05, 8.7689847e-01, 1.2305276e-01],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.9937356e-03, 9.4392991e-01, 5.2076314e-02],\n",
       "       [5.1952070e-01, 4.7261325e-01, 7.8659905e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.5700698e-02, 9.4545341e-01, 3.8845889e-02],\n",
       "       [4.0195680e-01, 5.8721924e-01, 1.0824002e-02],\n",
       "       [9.2889237e-01, 7.0421621e-02, 6.8604370e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.6194955e-03, 9.3578315e-01, 6.2597282e-02],\n",
       "       [9.4855255e-01, 5.0986245e-02, 4.6122036e-04],\n",
       "       [5.5446602e-05, 8.7976229e-01, 1.2018230e-01],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.2182532e-05, 8.6718005e-01, 1.3278781e-01],\n",
       "       [3.3908393e-05, 8.6843413e-01, 1.3153189e-01],\n",
       "       [9.7868557e-04, 9.2978895e-01, 6.9232382e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [3.2725956e-03, 9.4246936e-01, 5.4258022e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [7.3933817e-04, 9.2607147e-01, 7.3189214e-02],\n",
       "       [9.5623761e-01, 4.3384016e-02, 3.7836883e-04],\n",
       "       [3.8966353e-03, 9.4376141e-01, 5.2341960e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [6.6713907e-02, 9.0627271e-01, 2.7013402e-02],\n",
       "       [1.7736368e-04, 9.0324014e-01, 9.6582539e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [7.7925891e-02, 8.9629924e-01, 2.5774851e-02],\n",
       "       [9.1325921e-01, 8.5864648e-02, 8.7621092e-04],\n",
       "       [2.5566830e-03, 9.4037449e-01, 5.7068750e-02],\n",
       "       [1.3682838e-01, 8.4201163e-01, 2.1159939e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.3303966e-02, 9.4638968e-01, 4.0306307e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [2.3748179e-01, 7.4629283e-01, 1.6225487e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [1.8674418e-01, 7.9480803e-01, 1.8447818e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [4.9465486e-01, 4.9689639e-01, 8.4487880e-03],\n",
       "       [3.9643095e-05, 8.7212694e-01, 1.2783344e-01],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [2.1662537e-02, 9.4224477e-01, 3.6092762e-02],\n",
       "       [5.4059273e-01, 4.5201954e-01, 7.3877545e-03],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [2.6348612e-04, 9.1021830e-01, 8.9518294e-02],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04],\n",
       "       [9.5851719e-01, 4.1128345e-02, 3.5440997e-04]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B  \\\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11   \n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29   \n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03   \n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82   \n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26   \n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69   \n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40   \n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32   \n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53   \n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63   \n",
       "\n",
       "     Output  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "..      ...  \n",
       "875       1  \n",
       "876       1  \n",
       "877       1  \n",
       "878       2  \n",
       "879       0  \n",
       "\n",
       "[880 rows x 13 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 13)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"Output\"] == 2]\n",
    "data[data[\"Output\"] == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B  \\\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11   \n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29   \n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03   \n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82   \n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26   \n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69   \n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40   \n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32   \n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53   \n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63   \n",
       "\n",
       "     Output  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "..      ...  \n",
       "875       1  \n",
       "876       1  \n",
       "877       1  \n",
       "878       2  \n",
       "879       0  \n",
       "\n",
       "[880 rows x 13 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B  \\\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11   \n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29   \n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03   \n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82   \n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26   \n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69   \n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40   \n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32   \n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53   \n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63   \n",
       "\n",
       "     Output  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "..      ...  \n",
       "875       1  \n",
       "876       1  \n",
       "877       1  \n",
       "878       2  \n",
       "879       0  \n",
       "\n",
       "[880 rows x 13 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11\n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29\n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03\n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82\n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26\n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...\n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69\n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40\n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32\n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53\n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63\n",
       "\n",
       "[880 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>257</td>\n",
       "      <td>7.5</td>\n",
       "      <td>887</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>345</td>\n",
       "      <td>76.8</td>\n",
       "      <td>676</td>\n",
       "      <td>7.10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.27</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>320</td>\n",
       "      <td>12.3</td>\n",
       "      <td>676</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.96</td>\n",
       "      <td>11.05</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>201</td>\n",
       "      <td>8.3</td>\n",
       "      <td>697</td>\n",
       "      <td>7.48</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.30</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>201</td>\n",
       "      <td>89.9</td>\n",
       "      <td>454</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.21</td>\n",
       "      <td>2.01</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>69</td>\n",
       "      <td>4.4</td>\n",
       "      <td>507</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.39</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.03</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>282</td>\n",
       "      <td>7.0</td>\n",
       "      <td>401</td>\n",
       "      <td>7.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.67</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>339</td>\n",
       "      <td>7.5</td>\n",
       "      <td>475</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.98</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.54</td>\n",
       "      <td>7.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>295</td>\n",
       "      <td>7.0</td>\n",
       "      <td>433</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.49</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.01</td>\n",
       "      <td>6.36</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>295</td>\n",
       "      <td>4.8</td>\n",
       "      <td>327</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.42</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC     S    Zn    Fe    Cu     Mn     B\n",
       "290  257   7.5  887  7.50  0.29  0.79  4.50  0.26  2.84  0.67   6.76  0.12\n",
       "685  345  76.8  676  7.10  0.70  0.38  8.20  0.34  0.83  0.27   6.13  0.32\n",
       "547  320  12.3  676  7.90  0.50  0.59  9.95  0.38  8.24  0.96  11.05  0.27\n",
       "835  201   8.3  697  7.48  0.81  0.20  6.64  0.53  5.60  0.86  11.30  0.63\n",
       "798  201  89.9  454  7.43  0.53  0.78  6.94  0.55  8.21  2.01   7.54  0.17\n",
       "..   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...    ...   ...\n",
       "106   69   4.4  507  7.70  0.53  0.10  5.13  0.39  4.02  1.03  13.05  0.25\n",
       "270  282   7.0  401  7.71  0.48  0.18  2.50  0.24  2.48  0.67   7.64  0.10\n",
       "860  339   7.5  475  7.00  0.88  0.98  6.94  0.54  7.15  0.39  10.59  0.34\n",
       "435  295   7.0  433  7.41  0.35  0.49  8.45  0.48  2.05  2.01   6.36  0.61\n",
       "102  295   4.8  327  7.60  0.43  0.88  3.32  0.42  7.63  0.51   9.03  0.86\n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13244251, -0.32041273,  3.05921221, ..., -0.59400136,\n",
       "        -0.46581803, -0.8103952 ],\n",
       "       [ 1.25966853,  2.83400647,  1.37334967, ..., -1.45312198,\n",
       "        -0.61330069, -0.45349787],\n",
       "       [ 0.93943387, -0.10192482,  1.37334967, ...,  0.02886108,\n",
       "         0.53846867, -0.54272221],\n",
       "       ...,\n",
       "       [ 1.18281221, -0.32041273, -0.23261417, ..., -1.19538579,\n",
       "         0.43078292, -0.41780814],\n",
       "       [ 0.6191992 , -0.34317188, -0.5681887 , ...,  2.2840527 ,\n",
       "        -0.55945782,  0.06400325],\n",
       "       [ 0.6191992 , -0.44331218, -1.4151149 , ..., -0.93764961,\n",
       "         0.06558775,  0.51012491]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', decision_function_shape='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 1, 1, 2,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 2,\n",
       "       2, 1, 1, 2, 0, 1, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 0, 2, 1, 2, 2, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 1,\n",
       "       1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train_std, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predict = svm.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8068181818181818"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test_std, y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8068181818181818"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new, svm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_naive = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Naive Bayes is:  0.6590909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "ac = accuracy_score(y_test_new, y_naive)\n",
    "cm = confusion_matrix(y_test_new, y_naive)\n",
    "print('Accuracy for Naive Bayes is: ',ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8863636363636364"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test,y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8863636363636364"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgbclassifier = xgb.XGBClassifier(learning_rate = 0.2, n_estimators = 100, max_depth = 3, objective=\"multi:softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softmax', ...)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgbclassifier.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xgb = model_xgbclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgbclassifier.score(X_test, y_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693181818181818"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new, y_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>257</td>\n",
       "      <td>7.5</td>\n",
       "      <td>887</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.79</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>345</td>\n",
       "      <td>76.8</td>\n",
       "      <td>676</td>\n",
       "      <td>7.10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.27</td>\n",
       "      <td>6.13</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>320</td>\n",
       "      <td>12.3</td>\n",
       "      <td>676</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.95</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.96</td>\n",
       "      <td>11.05</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>201</td>\n",
       "      <td>8.3</td>\n",
       "      <td>697</td>\n",
       "      <td>7.48</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.30</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>201</td>\n",
       "      <td>89.9</td>\n",
       "      <td>454</td>\n",
       "      <td>7.43</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.21</td>\n",
       "      <td>2.01</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>69</td>\n",
       "      <td>4.4</td>\n",
       "      <td>507</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.39</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.03</td>\n",
       "      <td>13.05</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>282</td>\n",
       "      <td>7.0</td>\n",
       "      <td>401</td>\n",
       "      <td>7.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.67</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>339</td>\n",
       "      <td>7.5</td>\n",
       "      <td>475</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.98</td>\n",
       "      <td>6.94</td>\n",
       "      <td>0.54</td>\n",
       "      <td>7.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>295</td>\n",
       "      <td>7.0</td>\n",
       "      <td>433</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.49</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.01</td>\n",
       "      <td>6.36</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>295</td>\n",
       "      <td>4.8</td>\n",
       "      <td>327</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.42</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC     S    Zn    Fe    Cu     Mn     B\n",
       "290  257   7.5  887  7.50  0.29  0.79  4.50  0.26  2.84  0.67   6.76  0.12\n",
       "685  345  76.8  676  7.10  0.70  0.38  8.20  0.34  0.83  0.27   6.13  0.32\n",
       "547  320  12.3  676  7.90  0.50  0.59  9.95  0.38  8.24  0.96  11.05  0.27\n",
       "835  201   8.3  697  7.48  0.81  0.20  6.64  0.53  5.60  0.86  11.30  0.63\n",
       "798  201  89.9  454  7.43  0.53  0.78  6.94  0.55  8.21  2.01   7.54  0.17\n",
       "..   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...    ...   ...\n",
       "106   69   4.4  507  7.70  0.53  0.10  5.13  0.39  4.02  1.03  13.05  0.25\n",
       "270  282   7.0  401  7.71  0.48  0.18  2.50  0.24  2.48  0.67   7.64  0.10\n",
       "860  339   7.5  475  7.00  0.88  0.98  6.94  0.54  7.15  0.39  10.59  0.34\n",
       "435  295   7.0  433  7.41  0.35  0.49  8.45  0.48  2.05  2.01   6.36  0.61\n",
       "102  295   4.8  327  7.60  0.43  0.88  3.32  0.42  7.63  0.51   9.03  0.86\n",
       "\n",
       "[704 rows x 12 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "y_train_std=y_train\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy',random_state= 0)\n",
    "classifier.fit(X_train_std,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8295454545454546"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.92        78\n",
      "           1       0.88      0.88      0.88        88\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.87       176\n",
      "   macro avg       0.58      0.62      0.60       176\n",
      "weighted avg       0.82      0.87      0.84       176\n",
      "\n",
      "Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.68        78\n",
      "           1       0.67      0.73      0.70        88\n",
      "           2       0.23      0.30      0.26        10\n",
      "\n",
      "    accuracy                           0.66       176\n",
      "   macro avg       0.54      0.55      0.54       176\n",
      "weighted avg       0.67      0.66      0.66       176\n",
      "\n",
      "Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        78\n",
      "           1       0.88      0.89      0.88        88\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.88       176\n",
      "   macro avg       0.58      0.62      0.60       176\n",
      "weighted avg       0.83      0.88      0.85       176\n",
      "\n",
      "Support Vector Machine:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84        78\n",
      "           1       0.76      0.89      0.82        88\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.81       176\n",
      "   macro avg       0.54      0.57      0.55       176\n",
      "weighted avg       0.77      0.81      0.78       176\n",
      "\n",
      "Decision trees:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85        78\n",
      "           1       0.84      0.83      0.83        88\n",
      "           2       0.71      0.50      0.59        10\n",
      "\n",
      "    accuracy                           0.83       176\n",
      "   macro avg       0.79      0.73      0.76       176\n",
      "weighted avg       0.83      0.83      0.83       176\n",
      "\n",
      "Grad Boost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.93        78\n",
      "           1       0.90      0.89      0.89        88\n",
      "           2       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.89       176\n",
      "   macro avg       0.93      0.72      0.76       176\n",
      "weighted avg       0.90      0.89      0.88       176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBClassifier:\")\n",
    "print(classification_report(y_test_new,y_xgb))\n",
    "print(\"Naive Bayes:\")\n",
    "print(classification_report(y_test_new,y_naive))\n",
    "print(\"Random Forest:\")\n",
    "print(classification_report(y_test_new, predictions))\n",
    "print(\"Support Vector Machine:\")\n",
    "print(classification_report(y_test_new, svm_predict))\n",
    "print(\"Decision trees:\")\n",
    "print(classification_report(y_test_new, y_pred))\n",
    "print(\"Grad Boost:\")\n",
    "print(classification_report(y_test_new, y_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestClassifier = ensemble.RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = { \n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4, 5, 6 ,7 ,8, 9, 10],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(estimator=forestClassifier, param_grid=paramGrid, cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [4, 5, 6, 7, 8, 9, 10],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'n_estimators': [200, 300, 500]})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 4,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel = ensemble.RandomForestClassifier(criterion = 'gini',\n",
    " max_depth = 4,\n",
    " max_features = 'auto',\n",
    " n_estimators = 300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, n_estimators=300, random_state=42)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForestModel.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = randomForestModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        78\n",
      "           1       0.88      0.89      0.88        88\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.88       176\n",
      "   macro avg       0.58      0.62      0.60       176\n",
      "weighted avg       0.83      0.88      0.85       176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_new, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, n_estimators=300, random_state=42)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForestModel.fit(feature_matrix, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = randomForestModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_new, predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        78\n",
      "           1       0.88      0.89      0.88        88\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.88       176\n",
      "   macro avg       0.58      0.62      0.60       176\n",
      "weighted avg       0.83      0.88      0.85       176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\python 3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_new, predictions_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.93        78\n",
      "           1       0.90      0.89      0.89        88\n",
      "           2       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.89       176\n",
      "   macro avg       0.93      0.72      0.76       176\n",
      "weighted avg       0.90      0.89      0.88       176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_new, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
